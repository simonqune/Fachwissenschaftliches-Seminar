\chapter{Datengrundlage und Vorverarbeitung}
\section{3D-Datenformate und Datentypen}
Bei der semantischen Segmentierung von 3D-Daten spielen die zugrunde liegenden
Datenformate eine elementare Rolle. Diese bilden die Grundlage für die
Erfassung, Speicherung und Verarbeitung von 3D-Daten, die für die semantische
Segmentierung verwendet werden. In diesem Kapitel werden zwei verbreitete
3D-Datenformate dargestellt, die in der Forschung und Praxis einsatz finden.
\\Ein wichtiges 3D-Datenformat ist das Punktewolkenformat, das unter anderem
von LiDAR-Sensoren erzeugt wird. Eine Punktwolke ist eine Sammlungen von
3D-Punkten, die die Oberfläche von Objekten in der Umgebung darstellen. Jeder
Punkt einer Point-Cloud stellt dabei, im Falle eines LiDAR-Sensors, die
3D-Position eines reflektierten Lichtstrahles dar \cite{20222324}. Sie können
in verschiedenen Dateiformaten gespeichert werden, häufig geschieht dies im
binären LAS-Format, das für die Speicherung und den Austausch von
LiDAR-Punktwolken verwendet wird. Diese Formate ermöglichen die Speicherung von
großen Mengen an Punkten mit 3D-Koordinaten, Intensitätsinformationen und
weiteren Attributen, die zur semantischen Segmentierung verwendet werden können
\cite{elphcik1991using}. Das Punktwolkenformat hat aufgrund der großen Menge an
Punkten, die durch einen LiDAR-Sensor erzeugt werden, einen sehr hohen
Speicherbedarf, was zu einer rechenintensiven Verarbeitung der Daten führen
kann \cite{8578570}.

Neben dem Punktewolkenformat wird häufig auch das Voxel-Gitter Format für die
semantische Segmentierung verwendet. Voxel sind volumetrische Elemente, die den
Raum in einem dreidimensionalen Gitter unterteilen. Jedes Voxel enthält dabei
Informationen über Materialeigenschaften, Farbe oder Textur des Objektes an
diesem Punkt im Raum. Das Datenformat bietet dabei eine effektive Möglichkeit,
komplexe dreidimensionale Strukturen darzustellen und weiter zu analysieren.
Die Voxeldichte im Raum kann dabei frei gewählt werden und bestimmt so die
Auflösung und den Speicherbedarf des Datensatzes.\cite{niessner2013real,6706719}

\section{Vorverarbeitungsschritte wie Filterung, Normalenberechnung und Downsampling}

Die Vorverarbeitung von 3D-Daten ist ein wichtiger Schritt in der semantischen
Segmentierung, um die Qualität und Genauigkeit der Segmentierungsergebnisse zu
verbessern. In diesem Kapitel werden verschiedene Vorverarbeitungsschritte wie
Filterung, Normalenberechnung und Downsampling betrachtet, die oft in der
Praxis angewendet werden.

Zu Beginn wird häufig eine Filterung des Datensatzes durchgeführt. Dadurch kann
unerwünschtes Rauschen im Datensatz unterdrückt und dessen Qualität verbessert
werden. Dies kann durch verschiedene Filtertechniken erfolgen, wie zum Beispiel
durch Medianfilter, Gaußsche Filter \cite{9191237} oder region growing und
Bilateralfilter \cite{6460813}. Durch die Filterung und die damit einhergehende
Reduzierung des Rauschens kann die Qualität der Segmentierungsergebnisse
verbessert werden.

Downsampling: Beim Prozess des Downsamplings wird die Anzahl an Punkten
innerhalb einer Punktwolke reduziert, um die Verarbeitungsgeschwindigkeit und
den Speicherbedarf zu reduzieren. Dies kann durch verschiedene Verfahren
erfolgen. Eine sehr schnelle und recheneffiziente Methode ist hierbei das
sogenannte Random-Downsampling. Dabei wird ein bestehender Datensatz um eine
absolute oder relative Punktanzahl reduziert. Die wegfallenden Punkte werden
dabei zufällig ausgewählt. Nachteil ist hierbei, dass sich ursprünglich
ungleichmäßig abgetastete Bereich des Datensatzes noch vergrößern können, und
dadurch markante Merkmale in der Punktwolke verloren gehen \cite{987567547}.
Deshalb wird stattdessen häufig auf das Voxel-Grid-Downsampling
zurückgegriffen. Dabei wird der Raum, in der sich die Punktwolke befindet, in
gleichmäßige Voxel unterteilt. Daraufhin wird in jedem Voxel nur eine bestimmte
Anzahl an Punkten übernommen, die anderen werden aus dem Datensatz entfernt.
Welche Punkte übernommen werden kann dabei durch verschiedene Verfahren
bestimmt werden. Danach wird eine bestimmte Anzahl an Punkten, etwa durch ihre
nähe zum Mittelpunkt, ausgewählt und in das Voxel-Grid übernommen. Ziel des
Voxel-Grid-Downsamplings ist es, die Datenmenge zu reduzieren, während wichtige
strukturelle und semantische Informationen erhalten bleiben.

Normalenberechnung: Die Berechnung von Normalen ist ein optionaler Schritt, um
die geometrische Information der 3D-Daten zu erfassen. Dabei wird für jeden
Punkt oder Voxel des Datensatzes ein Normalenvektor bestimmt. Normalen sind
Vektoren, die senkrecht zur Oberfläche von Objekten in der Umgebung stehen und
gibt so Aufschluss über die Orientierung der Oberflächen der Szene.
SCHÄTZVERAHReN

\section{Datenannotation und Ground Truth-Erstellung}

Die Erstellung annotierter Daten und eines Ground Truths sind entscheidende
Schritte bei der semantischen Segmentierung von 3D-Daten. Da die semantische
Segmentierung auf Erweiterungen von neuronale Netze basiert, werden annotierte
Datensätze benötigt um diese zu trainieren. Dabei erfolgt eine manuelle
Klassifikation der Daten, indem semantische Labels oder Klasseninformationen
den 3D-Daten zugeordnet werden. Die Qualität und Genauigkeit der
Datenannotation sind von entscheidender Bedeutung für die Leistungsfähigkeit
der Modelle für die semantische Segmentierung.

Die Erstellung des Ground Truths umfasst dabei die Erzeugung von
referenzbasierten Segmentierungsergebnissen, die als Grundlage für das Training
und die Evaluation von Segmentierungsmodellen dienen. Der Ground Truth kann
sowohl manuell als auch automatisch erstellt werden, um die Zuverlässigkeit und
Vergleichbarkeit der Segmentierungsergebnisse sicherzustellen und die Qualität
der trainierten Modelle zu überprüfen. Dafür wird häufig der sogenannte IoU
(Intersection over Union) verwendet. Dabei wird die Anteil der überlappenden
Bereiche zwischen Ground Truth und Segmentierungsergebnis berechnet. Je höher
der Wert, desto besser ist das Segmentierungsergebnis.