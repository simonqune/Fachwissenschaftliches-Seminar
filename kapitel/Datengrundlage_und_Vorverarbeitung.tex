\chapter{Datengrundlage und Vorverarbeitung}
\section{3D-Datenformate und Datentypen}
Bei der semantischen Segmentierung von 3D-Daten spielen die zugrunde liegenden
Datenformate eine elementare Rolle. Diese bilden die Grundlage für die
Erfassung, Speicherung und Verarbeitung von 3D-Daten, die für die semantische
Segmentierung verwendet werden. In diesem Kapitel werden die beiden
verbreitetsten 3D-Datenformate und Datentypen untersucht, die in der Forschung
und Praxis eingesetz werden.

Ein wichtiges 3D-Datenformat ist das Punktewolkenformat, das vorallem durch
LiDAR-Sensoren erzeugt wird. Eine Punktwolke ist eine Sammlungen von
3D-Punkten, die die Oberfläche von Objekten in der Umgebung darstellen. Jeder
Punkt stellt dabei eine reflektierte Lichtwelle da. Sie können in verschiedenen
Dateiformaten gespeichert werden, häufig geschieht dies im binären LAS-Format,
das speziell für die Speicherung und den Austausch von LiDAR-Punktwolken
entwickelt wurde. Diese Formate ermöglichen die Speicherung von großen Mengen
an Punkten mit 3D-Koordinaten, Intensitätsinformationen und weiteren
Attributen, die zur semantischen Segmentierung verwendet werden können
\cite{.2020b}. Punktwolken weißen dadurch eine stark variable Punktdichte auf.
Dies lässt sich durch Faktoren, wie eine ungleichmäßige Abtastung des Raumes,
der Verdeckung von Objekten und der relativen Ausrichtung des Objektes zum
Sensor begründen. Das Punktwolkenformat hat aufgrund der großen Menge an
Punkten, die durch einen LiDAR-Sensor erzeugt werden, einen sehr hohen
Speicherbedarf, was zu einer rechenintensiven Verarbeitung der Daten führen
kann \cite{8578570}.

Neben Punktewolken werden auch 3D-Gitter oder Voxel-Daten oft für die
semantische Segmentierung verwendet. Voxel sind volumetrische Elemente, die den
Raum in einem dreidimensionalen Gitter unterteilen. Jedes Voxel enthält dabei
Informationen über Materialeigenschaften, Farbe oder Textur des Objektes an
diesem Punkt. Voxel-Daten können in verschiedenen Formaten gespeichert werden,
wie zum Beispiel das binäre OctoMap-Format oder dem ASCII-Format. Sie bieten
dabei eine effektive Möglichkeit, komplexe dreidimensionale Strukturen
darzustellen und weiter zu analysieren. Die Voxeldichte im Raum kann dabei frei
gewählt werden und bestimmt so die Auflösung und den Speicherbedarf des
Datensatzes.

\section{Vorverarbeitungsschritte wie Filterung, Normalenberechnung und Downsampling}

Die Vorverarbeitung von 3D-Daten ist ein wichtiger Schritt in der semantischen
Segmentierung, um die Qualität und Genauigkeit der Segmentierungsergebnisse zu
verbessern. In diesem Kapitel werden verschiedene Vorverarbeitungsschritte wie
Filterung, Normalenberechnung und Downsampling betrachtet, die oft in der
Praxis angewendet werden.

Zu Beginn wird häufig eine Filterung des Datensatzes durchgeführt. Dadurch kann
unerwünschtes Rauschen im Datensatz unterdrückt und dessen Qualität verbessert
werden. Dies kann durch verschiedene Filtertechniken erfolgen, wie zum Beispiel
durch Medianfilter, Gaußsche Filter \cite{9191237} oder region growing und
Bilateralfilter \cite{6460813}. Durch die Filterung und die damit einhergehende
Reduzierung des Rauschens kann die Qualität der Segmentierungsergebnisse
verbessert werden.

Downsampling: Beim Prozess des Downsamplings wird die Anzahl an Punkten
innerhalb einer Punktwolke reduziert, um die Verarbeitungsgeschwindigkeit und
den Speicherbedarf zu reduzieren. Dies kann durch verschiedene Verfahren
erfolgen. Eine sehr schnelle und recheneffiziente Methode ist hierbei das sogenannte
Random-Downsampling. Dabei wird ein bestehender Datensatz um eine absolute oder relative Punktanzahl 
reduziert. Die wegfallenden Punkte werden dabei zufällig
ausgewählt. Nachteil ist hierbei, dass sich ursprünglich ungleichmäßig
abgetastete Bereich des Datensatzes noch vergrößern können, und so größere Lücken in
den Punktwolken entstehen können. Deshalb wird stattdessen häufig auf das
Voxel-Grid-Downsampling zurückgegriffen. Dabei wird der Raum, in der sich die
Punktwolke befindet, in gleichmäßige Voxel unterteilt, wobei für jedes Voxel
der Schwerpunkt der darin enthaltenen Punkte berechnet wird. Danach wird eine
bestimmte Anzahl an Punkten, häufig durch ihre nähe zum Schwerpunkt, ausgewählt
und in das Voxel-Grid übernommen. Ziel des Voxel-Grid-Downsamplings ist es, die Datenmenge
zu reduzieren, während wichtige strukturelle und semantische Informationen
erhalten bleiben. ######SCHWERPUNTK FALSCH###########

Normalenberechnung: Die Berechnung von Normalen ist ein optionaler Schritt, um
die geometrische Information der 3D-Daten zu erfassen. Dabei wird für jeden
Punkt oder Voxel des Datensatzes ein Normalenvektor bestimmt. Normalen sind
Vektoren, die senkrecht zur Oberfläche von Objekten in der Umgebung stehen und
gibt so Aufschluss über die Orientierung der Oberflächen der Szene.
SCHÄTZVERAHReN

\section{Datenannotation und Ground Truth-Erstellung}

Das Erstellen annotierter Daten und eines Ground Truths sind entscheidende
Schritte bei der semantischen Segmentierung von 3D-Daten. Um neuronale Netze
für die Semantische Segmentierung zu trainieren, werden annotierte Datensätze
benötigt. Hierbei muss für einen Teil der Rohdaten eine manuelle Klassifikation
der Bilder erfolgen, indem semantische Labels oder Klasseninformationen manuell
oder automatisch den 3D-Daten zugeordnet werden. Die Qualität und Genauigkeit
der Datenannotation sind entscheidend für die Leistungsfähigkeit von
semantischen Segmentierungsalgorithmen. Die Erstellung des Ground Truths
umfasst die Erstellung von referenzbasierten Segmentierungsergebnissen, die als
Grundlage für das Training und die Evaluation von Segmentierungsmodellen
dienen. Die Ground Truth kann manuell oder automatisch erstellt werden, um die
Zuverlässigkeit und Vergleichbarkeit von Segmentierungsergebnissen
sicherzustellen und die Qualität von trainierten Modellen zu überprüfen. Die
Verwendung von Ground Truths ist jedoch nicht nur auf das Training beschränkt,
sondern kann auch für die Validierung und Bewertung von Modellen verwendet
werden.