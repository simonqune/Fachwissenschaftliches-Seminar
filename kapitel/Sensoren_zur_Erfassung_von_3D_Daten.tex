\chapter{Sensoren zur Erfassung von 3D-Daten}

\section{LiDAR-Sensoren}
LiDAR-Sensoren (Light Detection and Ranging) stellen eine weit verbreitete
Technologie zur Erfassung von 3D-Daten dar. Diese senden einen Laserstrahl aus,
welcher von Objekten in der Umgebung reflektiert wird. Die Distanz des
reflektierenden Objektes kann dabei über Time of Flight (TOF) oder über die
Phase der reflektierten Lichtwelle gemessen werden. Während TOF-LiDAR die
Distanz zum Objekt über eine Messung der Laufzeit des Laserstrahls bestimmt,
erfolgt die Entfernungsmessung beim phasenbasierten LiDAR über die Auswertung
der Phasenverschiebung der vom Objekt reflektierten Lichtwelle. Beide Methoden
können hochgenaue Entfernungen zu den reflektierenden Objekten erfassen. Sie
können detaillierte 3D-Punktwolken erzeugen, welche die Geometrie und räumliche
Verteilung von Objekten in der Umgebung darstellen. Zusätzlich lassen sich
LiDAR-Sensoren in Scanning-LiDAR und Non-Scanning-LiDAR untergliedern.
Non-Scanning-LiDAR nutzt dabei einen statischen Laserstrahl, während
Scanning-LiDAR einen sich bewegenden Laserstrahl nutzt und somit einen größeren
Arbeitsbereich abdecken kann. \cite{8529992}

\section{Tiefenkameras}
Unter dem Begriff Tiefenkamera lassen sich verschiedene Verfahren aufführen,
welche unterschiedliche Funktionsweisen besitzen, um Tiefeninformationen einer
Szene zu bestimmen. Im Bereich der semantische Segmentierung kommen besonders
Kamerasysteme, die auf Stereo-Vision, Time-of-Flight oder Structured Light
basieren, zum Einsatz \cite{20222324}.

Kamerasysteme, die auf dem Prinzip der Stereo-Vision basieren, werden als
Stereo Kameras bezeichnet. Bei diesen werden zwei räumlich getrennte Kameras
verwendet, die gemeinsam Bilder von derselben Szene aus zwei leicht
unterschiedlichen Perspektiven aufnehmen. Der dabei entstehende horizontale
Versatz der beiden Bilder wird als Disparität bezeichnet. Aus dieser lassen
sich Tiefeninformationen des betrachteten Objektes durch Triangulation
bestimmen \cite{8932817}.

Kameras die auf dem Time of Flight(TOF) Prinzip basieren, senden eine
Lichtwelle mit mehreren Modulationsfrequenzen im Infrarotbereich aus. Diese
wird, wie beim LiDAR-Sensor, von Objekten in der Umgebung reflektiert und
ermöglicht es Tiefeninformationen zu berechnen. Dabei wird sowohl die
Phasenverschiebung, als auch die Amplitude des reflektierten Signals gemessen
und so die Entfernung berechnet. Die meisten Verfahren nutzen dabei vier
Messungen um die Phasenverschiebung zu erkennen \cite{7035807,7025195}.
\\Tiefenkameras können zusätzlich mit einer RGB-Kamera ausgestattet sein und sind dann unter
dem Begriff RGB-D Kameras bekannt. Diese haben häufig eine höhere räumliche
Auflösung und sind in der Lage zusätzlich Farbinformationen aufzunehmen,
besitzen jedoch einen deutlich kleineren Arbeitsbereich \cite{9262651}. Ein
weiteres Verfahren basiert auf Structured Light. Bei diesem Verfahren wird ein
spezielles 2D-Muster auf das zu betrachtende Objekt projiziert. Aus dessen
Verzerrung lässt sich auf 3D-Information schließen \cite{7992709}.

\section{Passive und aktive Sensoren}
Die genannten Sensoren lassen sich zusätzlich in zwei Klassen unterteilen.
Aktive Sensoren, wie LIDAR-Sensoren, senden selbst Energie in Form von einer
Lichtwelle aus, um Informationen über die Umgebung zu sammeln. Im Gegensatz
dazu arbeiten passive Sensoren, wie die meisten Stereo Kamera-Systeme, ohne
aktiv Energie auszusenden. Sie nutzen lediglich das natürliche Licht, welches
von der Umgebung reflektiert wird. Der Vorteil von aktiven Sensoren besteht
darin, dass sie unabhängig von der Umgebungshelligkeit arbeiten und auch bei
Dunkelheit eingesetzt werden können. Passive Sensoren hingegen können bei
schlechten Lichtverhältnissen Schwierigkeiten haben, genaue Tiefeninformationen
zu liefern. Es ist jedoch anzumerken, dass passive Sensoren in der Regel
kostengünstiger sind und eine höhere räumliche Auflösung bieten können.
\cite{20222324}

\section{Auswahl von Sensoren für die semantische Segmentierung}

Die Auswahl geeigneter Sensoren für die Semantische Segmentierung ist von deren
Einsatzbereich abhängig. Dabei sind Faktoren wie die Umgebungsbedingungen,
Budget und gewünschte Ergebnisse ausschlaggebend. Je nach Szenario sind werden
unterschiedliche Anforderungen an die Genauigkeit, die räumliche Auflösung, die
Reichweite, sowie die Echtzeitfähigkeit gestellt. Anwendungen im Bereich der
autonomen Fahrzeuge benötigen beispielsweise häufig Sensoren mit hoher
Reichweite und Genauigkeit, während Anwendungen im Innenbereich möglicherweise
Sensoren mit höherer räumlicher Auflösung benötigen. Kann es im geplanten
Einsatzgebiet zu wechselnden Umgebungsbedingungen wie Wetter- oder
Beleuchtungsveränderungen kommen, sind oft aktive Sensoren geeigneter, um
unabhängig von äußeren Einflüssen funktionieren zu können. Durch ihre
unterschiedlichen Eigenschaften können Sensoren besser für die Erkennung
bestimmter Objekte geeignet sein. Außerdem ist das Budget bei der Sensorwahl zu
berücksichtigen, da sich die Sensoren stark in ihren Kosten unterscheiden
können.\cite{20222324}
