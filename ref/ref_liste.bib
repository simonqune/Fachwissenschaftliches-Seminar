@inproceedings{9102769,
  author    = {Yang, Kaihong and Bi, Sheng and Dong, Min},
  booktitle = {2020 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Lightningnet: Fast and Accurate Semantic Segmentation for Autonomous Driving Based on 3D LIDAR Point Cloud},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/ICME46284.2020.9102769}
}
  @inproceedings{8100085,
  author    = {Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation from Single Depth Images},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {5679-5688},
  doi       = {10.1109/CVPR.2017.602}
}
  @inproceedings{9423307,
  author    = {Pham, Tuan},
  booktitle = {2020 Applying New Technology in Green Buildings (ATiGB)},
  title     = {Semantic Road Segmentation using Deep Learning},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {45-48},
  doi       = {10.1109/ATiGB50996.2021.9423307}
}
  @inproceedings{9420573,
  author    = {Hosein Hamian, Mohammad and Beikmohammadi, Ali and Ahmadi, Ali and Nasersharif, Babak},
  booktitle = {2021 26th International Computer Conference, Computer Society of Iran (CSICC)},
  title     = {Semantic Segmentation of Autonomous Driving Images by the Combination of Deep Learning and Classical Segmentation},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/CSICC52343.2021.9420573}
}
  @inproceedings{8529992,
  author    = {Liu, Jingyun and Sun, Qiao and Fan, Zhe and Jia, Yudong},
  booktitle = {2018 IEEE 3rd Optoelectronics Global Conference (OGC)},
  title     = {TOF Lidar Development in Autonomous Vehicle},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {185-190},
  doi       = {10.1109/OGC.2018.8529992}
}
  @inproceedings{8932817,
  author    = {DANDIL, Emre and ÇEVİK, Kerim Kürşat},
  booktitle = {2019 3rd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)},
  title     = {Computer Vision Based Distance Measurement System using Stereo Camera View},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1-4},
  doi       = {10.1109/ISMSIT.2019.8932817}
}
  @inproceedings{7025195,
  author    = {Dalbah, Yosef and Rohr, Stephan and Wahl, Friedrich M.},
  booktitle = {2014 IEEE International Conference on Image Processing (ICIP)},
  title     = {Detection of dynamic objects for environment mapping by time-of-flight cameras},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {971-975},
  doi       = {10.1109/ICIP.2014.7025195}
}
  @inproceedings{7992709,
  author    = {Anwar, Inzamam and Lee, Sukhan},
  booktitle = {2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)},
  title     = {High performance stand-alone structured light 3D camera for smart manipulators},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {192-195},
  doi       = {10.1109/URAI.2017.7992709}
}
  @inproceedings{9262651,
  author    = {da Silva Neto, José Gomes and da Lima Silva, Pedro Jorge and Figueredo, Filipe and Teixeira, João Marcelo Xavier Natário and Teichrieb, Veronica},
  booktitle = {2020 22nd Symposium on Virtual and Augmented Reality (SVR)},
  title     = {Comparison of RGB-D sensors for 3D reconstruction},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {252-261},
  doi       = {10.1109/SVR51698.2020.00046}
}
  @inproceedings{8578570,
  author    = {Zhou, Yin and Tuzel, Oncel},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {4490-4499},
  doi       = {10.1109/CVPR.2018.00472}
}
  @inproceedings{9191237,
  author    = {Thurnhofer-Hemsi, Karl and López-Rubio, Ezequiel and Roé-Vellvé, Núria and Deka, Lipika},
  booktitle = {2020 IEEE International Conference on Image Processing (ICIP)},
  title     = {Super-Resolution of 3D MRI Corrupted by Heavy Noise With the Median Filter Transform},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {3015-3019},
  doi       = {10.1109/ICIP40778.2020.9191237}
}
  @inproceedings{6460813,
  author    = {Chen, Li and Lin, Hui and Li, Shutao},
  booktitle = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
  title     = {Depth image enhancement for Kinect using region growing and bilateral filter},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {3070-3073},
  doi       = {}
}
  @misc{11262015,
  abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.  This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  author   = {O'Shea, Keiron and Nash, Ryan},
  date     = {11/26/2015},
  title    = {An Introduction to Convolutional Neural Networks},
  url      = {https://arxiv.org/pdf/1511.08458.pdf},
  urldate  = {5/8/2023},
  file     = {acf529c2-9ceb-4247-af2b-3d18386a3378:C\:\\Users\\kuhns\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\ffp672gkolya7su4r7atzhmr8k0hhpf6fnt87hoagh3kec\\Citavi Attachments\\acf529c2-9ceb-4247-af2b-3d18386a3378.pdf:pdf;8d823a62-a4e7-4d43-bed5-6ba57d4d8b17:C\:\\Users\\kuhns\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\ffp672gkolya7su4r7atzhmr8k0hhpf6fnt87hoagh3kec\\Remote Attachments\\8d823a62-a4e7-4d43-bed5-6ba57d4d8b17.pdf:pdf}
}
@inproceedings{7298965,
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Fully convolutional networks for semantic segmentation},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {3431-3440},
  doi       = {10.1109/CVPR.2015.7298965}
}
  @inproceedings{8237584,
  author    = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Mask R-CNN},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {2980-2988},
  doi       = {10.1109/ICCV.2017.322}
}
  @article{7803544,
  author  = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  year    = {2017},
  volume  = {39},
  number  = {12},
  pages   = {2481-2495},
  doi     = {10.1109/TPAMI.2016.2644615}
}

@article{987567547,
  abstract = {To investigate forest resources, it is necessary to identify the tree species. However, it is a challenge to identify tree species using 3D point clouds of trees collected by light detection and ranging (LiDAR). PointNet++, a point cloud deep learning network, can effectively classify 3D objects. It is important to establish high-quality individual tree point cloud datasets when applying PointNet++ to identifying tree species. However, there are different data processing methods to produce sample datasets, and the processes are tedious. In this study, we suggest how to select the appropriate method by designing comparative experiments. We used the backpack laser scanning (BLS) system to collect point cloud data for a total of eight tree species in three regions. We explored the effect of tree height on the classification accuracy of tree species by using different point cloud normalization methods and analyzed the effect of leaf point clouds on classification accuracy by separating the leaves and wood of individual tree point clouds. Five downsampling methods were used: farthest point sampling (FPS), K-means, random, grid average sampling, and nonuniform grid sampling (NGS). Data with different sampling points were designed for the experiments. The results show that the tree height feature is unimportant when using point cloud deep learning methods for tree species classification. For data collected in a single season, the leaf point cloud has little effect on the classification accuracy. The two suitable point cloud downsampling methods we screened were FPS and NGS, and the deep learning network could provide the most accurate tree species classification when the number of individual tree point clouds was in the range of 2048--5120. Our study further illustrates that point-based end-to-end deep learning methods can be used to classify tree species and identify individual tree point clouds. Combined with the low-cost and high-efficiency BLS system, it can effectively improve the efficiency of forest resource surveys.},
  author   = {Liu, Bingjie and Chen, Shuxin and Huang, Huaguo and Tian, Xin},
  year     = {2022},
  title    = {Tree Species Classification of Backpack Laser Scanning Data Using the PointNet++ Point Cloud Deep Learning Method},
  url      = {https://www.mdpi.com/2072-4292/14/15/3809},
  pages    = {3809},
  volume   = {14},
  number   = {15},
  issn     = {2072-4292},
  journal  = {Remote Sensing},
  doi      = {10.3390/rs14153809},
  file     = {b86129cd-519d-4bf1-a016-1ed17021db07:C\:\\Users\\kuhns\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\ffp672gkolya7su4r7atzhmr8k0hhpf6fnt87hoagh3kec\\Citavi Attachments\\b86129cd-519d-4bf1-a016-1ed17021db07.pdf:pdf}
}
@article{CGV-079,
  url     = {http://dx.doi.org/10.1561/0600000079},
  year    = {2020},
  volume  = {12},
  journal = {Foundations and Trends® in Computer Graphics and Vision},
  title   = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art},
  doi     = {10.1561/0600000079},
  issn    = {1572-2740},
  number  = {1–3},
  pages   = {1-308},
  author  = {Joel Janai and Fatma Güney and Aseem Behl and Andreas Geiger}
}
@inproceedings{8206396,
  author    = {Ha, Qishen and Watanabe, Kohei and Karasawa, Takumi and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {MFNet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {5108-5115},
  doi       = {10.1109/IROS.2017.8206396}
}
<@article{20222324,
  title     = {Multimodal Semantic Segmentation in Autonomous Driving: A Review of Current Approaches and Future Perspectives},
  volume    = {10},
  issn      = {2227-7080},
  url       = {http://dx.doi.org/10.3390/technologies10040090},
  doi       = {10.3390/technologies10040090},
  number    = {4},
  journal   = {Technologies},
  publisher = {MDPI AG},
  author    = {Rizzoli, Giulia and Barbato, Francesco and Zanuttigh, Pietro},
  year      = {2022},
  month     = {6},
  pages     = {90}
}
  @inproceedings{7035807,
  author    = {Kerl, Christian and Souiai, Mohamed and Sturm, Jürgen and Cremers, Daniel},
  booktitle = {2014 2nd International Conference on 3D Vision},
  title     = {Towards Illumination-Invariant 3D Reconstruction Using ToF RGB-D Cameras},
  year      = {2014},
  volume    = {1},
  number    = {},
  pages     = {39-46},
  doi       = {10.1109/3DV.2014.62}
}
  @article{elphcik1991using,
  title   = {Using the LAS format-part I.[Well log files]},
  author  = {Elphcik, RY},
  journal = {Geobyte;(United States)},
  volume  = {6},
  number  = {6},
  year    = {1991}
}
@inproceedings{6706719,
  author    = {Orts-Escolano, Sergio and Morell, Vicente and García-Rodríguez, José and Cazorla, Miguel},
  booktitle = {The 2013 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Point cloud data filtering and downsampling using growing neural gas},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {1-8},
  doi       = {10.1109/IJCNN.2013.6706719}
}
  @article{niessner2013real,
  title     = {Real-time 3D reconstruction at scale using voxel hashing},
  author    = {Nie{\ss}ner, Matthias and Zollh{\"o}fer, Michael and Izadi, Shahram and Stamminger, Marc},
  journal   = {ACM Transactions on Graphics (ToG)},
  volume    = {32},
  number    = {6},
  pages     = {1--11},
  year      = {2013},
  publisher = {ACM New York, NY, USA}
}
@article{HAN2017103,
  title    = {A review of algorithms for filtering the 3D point cloud},
  journal  = {Signal Processing: Image Communication},
  volume   = {57},
  pages    = {103-112},
  year     = {2017},
  issn     = {0923-5965},
  doi      = {https://doi.org/10.1016/j.image.2017.05.009},
  url      = {https://www.sciencedirect.com/science/article/pii/S0923596517300930},
  author   = {Xian-Feng Han and Jesse S. Jin and Ming-Jie Wang and Wei Jiang and Lei Gao and Liping Xiao},
  keywords = {3D point cloud, Filtering methods, Feature-preserving, Noise reduction},
  abstract = {In recent years, 3D point cloud has gained increasing attention as a new representation for objects. However, the raw point cloud is often noisy and contains outliers. Therefore, it is crucial to remove the noise and outliers from the point cloud while preserving the features, in particular, its fine details. This paper makes an attempt to present a comprehensive analysis of the state-of-the-art methods for filtering point cloud. The existing methods are categorized into seven classes, which concentrate on their common and obvious traits. An experimental evaluation is also performed to demonstrate robustness, effectiveness and computational efficiency of several methods used widely in practice.}
}
@article{8309343,
  author  = {Zhang, Zhengxin and Liu, Qingjie and Wang, Yunhong},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  title   = {Road Extraction by Deep Residual U-Net},
  year    = {2018},
  volume  = {15},
  number  = {5},
  pages   = {749-753},
  doi     = {10.1109/LGRS.2018.2802944}
}
@inproceedings{Girshick_2015_ICCV,
  author    = {Girshick, Ross},
  title     = {Fast R-CNN},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {December},
  year      = {2015}
}
@inproceedings{NIPS2017_d8bf84be,
  author    = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf},
  volume    = {30},
  year      = {2017}
}
@misc{ronneberger2015unet,
  title         = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  author        = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  year          = {2015},
  eprint        = {1505.04597},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{çiçek20163d,
  title         = {3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation},
  author        = {Özgün Çiçek and Ahmed Abdulkadir and Soeren S. Lienkamp and Thomas Brox and Olaf Ronneberger},
  year          = {2016},
  eprint        = {1606.06650},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{2233445566,
author = {Jung-Ping Liu and Ming-Hsuan Wu and Peter W. M. Tsang},
journal = {Opt. Express},
keywords = {Digital micromirror devices; Holographic displays; Liquid crystal modulators; Speckle noise; Speckle reduction; Three dimensional imaging},
number = {17},
pages = {24526--24537},
publisher = {Optica Publishing Group},
title = {3D display by binary computer-generated holograms with localized random down-sampling and adaptive intensity accumulation},
volume = {28},
month = {8},
year = {2020},
url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-28-17-24526},
doi = {10.1364/OE.399011},
abstract = {In this paper, we proposed a new technique to realize a high-quality three-dimensional (3D) display by using binary holograms. First, we applied a localized random down-sampling (LRDS) mask to down-sample the object function and generated a binary CGH by direct sign-thresholding. Subsequently, we devised the display by adaptive intensity accumulation (AIA). In AIA, multiple CGHs of the same object are generated. However, selective sampling points of the same scene are removed according to the reconstructed image of previous binary CGHs as the second and more binary CGHs are generated. Finally, these holograms are sequentially displayed on a fast spatial light modulator, a digital micromirror device (DMD). Thus, a high-quality 3D image is reconstructed without artifacts and speckle noise.},
}
@INPROCEEDINGS{6375034,
  author={Ioannou, Yani and Taati, Babak and Harrap, Robin and Greenspan, Michael},
  booktitle={2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission}, 
  title={Difference of Normals as a Multi-scale Operator in Unorganized Point Clouds}, 
  year={2012},
  volume={},
  number={},
  pages={501-508},
  doi={10.1109/3DIMPVT.2012.12}
}
@article{BROSTOW200988,
title = {Semantic object classes in video: A high-definition ground truth database},
journal = {Pattern Recognition Letters},
volume = {30},
number = {2},
pages = {88-97},
year = {2009},
note = {Video-based Object and Event Analysis},
issn = {0167-8655},
doi = {10.1016/j.patrec.2008.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167865508001220},
author = {Gabriel J. Brostow and Julien Fauqueur and Roberto Cipolla},
keywords = {Object recognition, Video database, Video understanding, Semantic segmentation, Label propagation},
abstract = {Visual object analysis researchers are increasingly experimenting with video, because it is expected that motion cues should help with detection, recognition, and other analysis tasks. This paper presents the Cambridge-driving Labeled Video Database (CamVid) as the first collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes. The database addresses the need for experimental data to quantitatively evaluate emerging algorithms. While most videos are filmed with fixed-position CCTV-style cameras, our data was captured from the perspective of a driving automobile. The driving scenario increases the number and heterogeneity of the observed object classes. Over 10min of high quality 30Hz footage is being provided, with corresponding semantically labeled images at 1Hz and in part, 15Hz. The CamVid Database offers four contributions that are relevant to object analysis researchers. First, the per-pixel semantic segmentation of over 700 images was specified manually, and was then inspected and confirmed by a second person for accuracy. Second, the high-quality and large resolution color video images in the database represent valuable extended duration digitized footage to those interested in driving scenarios or ego-motion. Third, we filmed calibration sequences for the camera color response and intrinsics, and computed a 3D camera pose for each frame in the sequences. Finally, in support of expanding this or other databases, we present custom-made labeling software for assisting users who wish to paint precise class-labels for other images and videos. We evaluate the relevance of the database by measuring the performance of an algorithm from each of three distinct domains: multi-class object recognition, pedestrian detection, and label propagation.}
}
@InProceedings{Zlateski_2018_CVPR,
author = {Zlateski, Aleksandar and Jaroensri, Ronnachai and Sharma, Prafull and Durand, Frédo},
title = {On the Importance of Label Quality for Semantic Segmentation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {6},
year = {2018}
}
@ARTICLE{9000872,
author={Feng, Di and Haase-Schütz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Gläser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
journal={IEEE Transactions on Intelligent Transportation Systems}, 
title={Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges}, 
year={2021},
volume={22},
number={3},
pages={1341-1360},
doi={10.1109/TITS.2020.2972974}
}
@article{987654321,
author = {Nemoto, Takafumi and Futakami, Natsumi and Yagi, Masamichi and Kumabe, Atsuhiro and Takeda, Atsuya and Kunieda, Etsuo and Shigematsu, Naoyuki},
title = {Efficacy evaluation of 2D, 3D U-Net semantic segmentation and atlas-based segmentation of normal lungs excluding the trachea and main bronchi},
journal = {Journal of Radiation Research},
volume = {61},
number = {2},
pages = {257-264},
year = {2020},
month = {2},
issn = {1349-9157},
doi = {10.1093/jrr/rrz086},
url = {https://doi.org/10.1093/jrr/rrz086},
eprint = {https://academic.oup.com/jrr/article-pdf/61/2/257/33292180/rrz086.pdf},
}
















